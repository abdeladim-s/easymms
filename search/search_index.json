{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"EasyMMS API Reference","text":""},{"location":"#easymms.models","title":"easymms.models","text":""},{"location":"#easymms.models.tts","title":"tts","text":"<p>This file contains a class definition to use the TTS models from Meta's Massively Multilingual Speech (MMS) project</p>"},{"location":"#easymms.models.tts.TTSModel","title":"TTSModel","text":"<pre><code>TTSModel(lang, model_dir=None, log_level=logging.INFO)\n</code></pre> <p>TTS class model</p> <p>Example usage:</p> <pre><code>from easymms.models.tts import TTSModel\n\ntts = TTSModel('eng')\nres = tts.synthesize(\"This is a simple example\")\ntts.save(res)\n</code></pre> <p>Use a TTS model by its language ISO ID. The model will be downloaded automatically</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>TTS model language</p> required <code>log_level</code> <code>int</code> <p>log level</p> <code>logging.INFO</code> Source code in <code>easymms/models/tts.py</code> <pre><code>def __init__(self,\n             lang: str,\n             model_dir: str = None,\n             log_level: int = logging.INFO):\n\"\"\"\n    Use a TTS model by its language ISO ID.\n    The model will be downloaded automatically\n\n    :param lang: TTS model language\n    :param log_level: log level\n    \"\"\"\n    self.log_level = log_level\n    self.lang = lang\n    set_log_level(log_level)\n    # check if models_dir is provided\n    if model_dir is not None:\n        # verify if all files exist\n        model_dir_path = Path(model_dir)\n    else:\n        model_dir_path = self._download_tts_model_files(lang)\n\n    self.cp = model_dir_path / \"G_100000.pth\"\n    assert self.cp.exists(), f\"G_100000.pth not found in {str(model_dir_path)}\"\n    self.config = model_dir_path / \"config.json\"\n    assert self.config.exists(), f\"config.json not found in {str(model_dir_path)}\"\n    self.vocab = model_dir_path / \"vocab.txt\"\n    assert self.vocab.exists(), f\"vocab.txt not found in {str(model_dir_path)}\"\n    self.uroman_dir_path = None\n\n    self._setup()\n</code></pre>"},{"location":"#easymms.models.tts.TTSModel.get_supported_langs","title":"get_supported_langs  <code>staticmethod</code>","text":"<pre><code>get_supported_langs()\n</code></pre> <p>Helper function to get supported ISO 693-3 languages by the TTS models Source https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of supported languages</p> Source code in <code>easymms/models/tts.py</code> <pre><code>@staticmethod\ndef get_supported_langs() -&gt; List[str]:\n\"\"\"\n    Helper function to get supported ISO 693-3 languages by the TTS models\n    Source &lt;https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html&gt;\n    :return: list of supported languages\n    \"\"\"\n    with open(constants.MMS_LANGS_FILE) as f:\n        data = json.load(f)\n        return [key for key in data if data[key]['TTS']]\n</code></pre>"},{"location":"#easymms.models.tts.TTSModel.synthesize","title":"synthesize","text":"<pre><code>synthesize(txt, device=None)\n</code></pre> <p>Synthesizes the text provided as input.</p> <p>Parameters:</p> Name Type Description Default <code>txt</code> <code>str</code> <p>Text</p> required <code>lang</code> <p>Language</p> required <code>device</code> <p>Pytorch device (cpu/cuda)</p> <code>None</code> <p>Returns:</p> Type Description <p>Tuple(data, sample_rate)</p> Source code in <code>easymms/models/tts.py</code> <pre><code>def synthesize(self, txt: str, device=None):\n\"\"\"\n     Synthesizes the text provided as input.\n\n    :param txt: Text\n    :param lang: Language\n    :param device: Pytorch device (cpu/cuda)\n    :return: Tuple(data, sample_rate)\n    \"\"\"\n    cwd = os.getcwd()\n    os.chdir(constants.VITS_DIR)\n    from utils import get_hparams_from_file, load_checkpoint\n    from models import SynthesizerTrn\n    os.chdir(constants.FAIRSEQ_DIR)\n    from examples.mms.tts.infer import TextMapper\n    os.chdir(cwd)\n    if device is None:\n        if torch.cuda.is_available():\n            device = 'cuda'\n        else:\n            device = 'cpu'\n    else:\n        assert device in ['cpu', 'cuda']\n\n    text_mapper = TextMapper(str(self.vocab))\n    hps = get_hparams_from_file(str(self.config))\n    net_g = SynthesizerTrn(\n        len(text_mapper.symbols),\n        hps.data.filter_length // 2 + 1,\n        hps.train.segment_size // hps.data.hop_length,\n        **hps.model)\n    net_g.to(device)\n    _ = net_g.eval()\n    g_pth = self.cp\n    logger.info(f\"loading {g_pth} ...\")\n    _ = load_checkpoint(g_pth, net_g, None)\n    logger.info(f\"text: {txt}\")\n    is_uroman = hps.data.training_files.split('.')[-1] == 'uroman'\n    if is_uroman:\n        uroman_pl = str(self.uroman_dir_path / \"uroman.pl\")\n        txt = text_mapper.uromanize(txt, uroman_pl)\n        logger.info(f\"uroman text: {txt}\")\n    txt = txt.lower()\n    txt = text_mapper.filter_oov(txt, lang=self.lang)\n    stn_tst = text_mapper.get_text(txt, hps)\n    # inference\n    with torch.no_grad():\n        x_tst = stn_tst.unsqueeze(0).to(device)\n        x_tst_lengths = torch.LongTensor([stn_tst.size(0)]).to(device)\n        hyp = net_g.infer(\n            x_tst, x_tst_lengths, noise_scale=.667,\n            noise_scale_w=0.8, length_scale=1.0\n        )[0][0, 0].cpu().float().numpy()\n\n    return hyp, hps.data.sampling_rate\n</code></pre>"},{"location":"#easymms.models.tts.TTSModel.save","title":"save","text":"<pre><code>save(tts_data, out_file='out.wav')\n</code></pre> <p>Saves the results of the <code>synthesize</code> function to a file</p> <p>Parameters:</p> Name Type Description Default <code>tts_data</code> <code>Tuple</code> <p>tts_data: a tuple of <code>wav data array</code> and <code>sample rate</code></p> required <code>out_file</code> <p>output file path</p> <code>'out.wav'</code> <p>Returns:</p> Type Description <code>Path</code> <p>out_file absolute path</p> Source code in <code>easymms/models/tts.py</code> <pre><code>def save(self, tts_data: Tuple, out_file='out.wav') -&gt; Path:\n\"\"\"\n    Saves the results of the `synthesize` function to a file\n\n    :param tts_data: tts_data: a tuple of `wav data array` and `sample rate`\n    :param out_file: output file path\n\n    :return: out_file absolute path\n    \"\"\"\n    set_log_level(self.log_level)\n    logger.info(f\"Saving audio file to {out_file}\")\n    sf.write(out_file, tts_data[0], tts_data[1])\n    return out_file\n</code></pre>"},{"location":"#easymms.models.alignment","title":"alignment","text":"<p>This file contains a simple class to use the AlignmentModel model from the MMS project More info can be found here: https://github.com/facebookresearch/fairseq/tree/main/examples/mms/data_prep</p>"},{"location":"#easymms.models.alignment.AlignmentModel","title":"AlignmentModel","text":"<pre><code>AlignmentModel(\n    model=None,\n    dictionary=None,\n    uroman_dir=None,\n    log_level=logging.INFO,\n)\n</code></pre> <p>MMS Alignment algorithm Example usage:</p> <pre><code>from easymms.models.alignment import AlignmentModel\n\nalign_model = AlignmentModel()\ntranscriptions = align_model.align('/home/su/code/easymms/assets/eng_1.mp3',\n                                   transcript=[\"segment 1\", \"segment 2\"],\n                                   lang='eng')\nfor transcription in transcriptions:\n    for segment in transcription:\n        print(f\"{segment['start_time']} -&gt; {segment['end_time']}: {segment['text']}\")\n</code></pre> Source code in <code>easymms/models/alignment.py</code> <pre><code>def __init__(self,\n             model: str = None,\n             dictionary: str = None,\n             uroman_dir: str = None,\n             log_level: int = logging.INFO):\n\n    set_log_level(log_level)\n    assert shutil.which(\"perl\") is not None, \"To use the alignment algorithm you will need uroman \" \\\n                                             \"&lt;https://github.com/isi-nlp/uroman&gt; which is written in perl \" \\\n                                             \"please install perl first &lt;https://www.perl.org/get.html&gt;\"\n    if uroman_dir is not None:\n        self.uroman_dir_path = Path(uroman_dir)\n    else:\n        self.uroman_dir_path = easymms_utils.get_uroman()\n\n    if model is not None:\n        self.model_path = Path(model)\n    else:\n        self.model_path = Path(PACKAGE_DATA_DIR) / \"ctc_alignment_mling_uroman_model.pt\"\n\n    if dictionary is not None:\n        self.dictionary_path = Path(dictionary)\n    else:\n        self.dictionary_path = Path(PACKAGE_DATA_DIR) / \"ctc_alignment_mling_uroman_model.dict\"\n\n    self.model, self.dictionary = self._load_model_dict()\n\n    # clone Fairseq\n    easymms_utils.clone(constants.FAIRSEQ_URL, constants.FAIRSEQ_DIR)\n    sys.path.append(str(constants.FAIRSEQ_DIR.resolve()))\n</code></pre>"},{"location":"#easymms.models.alignment.AlignmentModel.align","title":"align","text":"<pre><code>align(media_file, transcript, lang, device=None)\n</code></pre> <p>Takes a media file, transcription segments and the lang and returns a list of dicts in the following format [{     'start_time': ...     'end_time': ...,     'text': ...,     'duration': ... }, ...]</p> <p>Parameters:</p> Name Type Description Default <code>media_file</code> <code>str</code> <p>the path of the media file, should be wav</p> required <code>transcript</code> <code>List[str]</code> <p>list of segments</p> required <code>lang</code> <code>str</code> <p>language ISO code</p> required <code>device</code> <code>str</code> <p>'cuda' or 'cpu'</p> <code>None</code> <p>Returns:</p> Type Description <code>List[dict]</code> <p>list of transcription timestamps</p> Source code in <code>easymms/models/alignment.py</code> <pre><code>def align(self,\n          media_file: str,\n          transcript: List[str],\n          lang: str,\n          device: str = None) -&gt; List[dict]:\n\"\"\"\n    Takes a media file, transcription segments and the lang and returns a list of dicts in the following format\n    [{\n        'start_time': ...\n        'end_time': ...,\n        'text': ...,\n        'duration': ...\n    }, ...]\n\n    :param media_file: the path of the media file, should be wav\n    :param transcript: list of segments\n    :param lang: language ISO code\n    :param device: 'cuda' or 'cpu'\n    :return: list of transcription timestamps\n    \"\"\"\n    # import\n    import os\n    cwd = os.getcwd()\n    os.chdir(constants.FAIRSEQ_DIR)\n    from examples.mms.data_prep.align_and_segment import get_alignments\n    from examples.mms.data_prep.align_utils import get_uroman_tokens, get_spans\n    from examples.mms.data_prep.text_normalization import text_normalize\n    os.chdir(cwd)\n\n    if device is None:\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    model = self.model.to(device)\n    res = []\n    logger.info(f\"Aligning file {media_file} ...\")\n    norm_transcripts = [text_normalize(line.strip(), lang) for line in transcript]\n    tokens = get_uroman_tokens(norm_transcripts, str(self.uroman_dir_path.resolve()), lang)\n\n    segments, stride = get_alignments(\n        media_file,\n        tokens,\n        model,\n        self.dictionary,\n        use_star=False,\n    )\n    # Get spans of each line in input text file\n    spans = get_spans(tokens, segments)\n\n    for i, t in enumerate(transcript):\n        span = spans[i]\n        seg_start_idx = span[0].start\n        seg_end_idx = span[-1].end\n        audio_start_sec = seg_start_idx * stride / 1000\n        audio_end_sec = seg_end_idx * stride / 1000\n        res.append({'start_time': audio_start_sec,\n                    'end_time': audio_end_sec,\n                    'text': t,\n                    'duration': audio_end_sec - audio_start_sec})\n\n    return res\n</code></pre>"},{"location":"#easymms.models.asr","title":"asr","text":"<p>This file contains a class definition to use the ASR models from Meta's Massively Multilingual Speech (MMS) project</p>"},{"location":"#easymms.models.asr.ASRModel","title":"ASRModel","text":"<pre><code>ASRModel(model, log_level=logging.INFO)\n</code></pre> <p>MMS ASR class model</p> <p>Example usage:</p> <pre><code>from easymms.models.asr import ASRModel\n\nasr = ASRModel(model='/path/to/mms/model')\nfiles = ['path/to/media_file_1', 'path/to/media_file_2']\ntranscriptions = asr.transcribe(files, lang='eng', align=False)\nfor i, transcription in enumerate(transcriptions):\n    print(f\"&gt;&gt;&gt; file {files[i]}\")\n    print(transcription)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>path to the asr model https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr</p> required <code>log_level</code> <code>int</code> <p>log level</p> <code>logging.INFO</code> Source code in <code>easymms/models/asr.py</code> <pre><code>def __init__(self,\n             model: str,\n             log_level: int = logging.INFO):\n\"\"\"\n    :param model: path to the asr model &lt;https://github.com/facebookresearch/fairseq/tree/main/examples/mms#asr&gt;\n    :param log_level: log level\n    \"\"\"\n    set_log_level(log_level)\n    self.cfg = constants.CFG.copy()\n    self.model = Path(model)\n    self.cfg['common_eval']['path'] = str(self.model.resolve())\n\n    self.tmp_dir = tempfile.TemporaryDirectory()\n    self.tmp_dir_path = Path(self.tmp_dir.name)\n    atexit.register(self._cleanup)\n\n    self.wer = None\n</code></pre>"},{"location":"#easymms.models.asr.ASRModel.transcribe","title":"transcribe","text":"<pre><code>transcribe(\n    media_files,\n    lang=\"eng\",\n    device=None,\n    align=False,\n    timestamps_type=\"segment\",\n    max_segment_len=27,\n    cfg=None,\n)\n</code></pre> <p>Transcribes a list of media files provided as inputs</p> <p>Parameters:</p> Name Type Description Default <code>media_files</code> <code>List[str]</code> <p>list of media files (video/audio), in whichever format supported by ffmpeg</p> required <code>lang</code> <code>str</code> <p>the language of the media</p> <code>'eng'</code> <code>device</code> <code>str</code> <p>Pytorch device (<code>cuda</code>, <code>cpu</code> or <code>tpu</code>)</p> <code>None</code> <code>align</code> <code>bool</code> <p>if True the alignment model will be used to generate the timestamps, otherwise you will get raw text from the MMS model</p> <code>False</code> <code>timestamps_type</code> <code>str</code> <p>Once of (<code>segment</code>, <code>word</code> or <code>char</code>) if <code>align</code> is set to True, this will be used to fragment the raw text</p> <code>'segment'</code> <code>max_segment_len</code> <code>int</code> <p>the maximum length of the fragmented segments</p> <code>27</code> <code>cfg</code> <code>dict</code> <p>configuration dict in case you want to use a custom configuration, see CFG</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[str], List[dict]]</code> <p>List of transcription text in the same order as input files</p> Source code in <code>easymms/models/asr.py</code> <pre><code>def transcribe(self,\n               media_files: List[str],\n               lang: str = 'eng',\n               device: str = None,\n               align: bool = False,\n               timestamps_type: str = 'segment',\n               max_segment_len: int = 27,\n               cfg: dict = None) -&gt; Union[List[str], List[dict]]:\n\"\"\"\n    Transcribes a list of media files provided as inputs\n\n    :param media_files: list of media files (video/audio), in whichever format supported by ffmpeg\n    :param lang: the language of the media\n    :param device: Pytorch device (`cuda`, `cpu` or `tpu`)\n    :param align: if True the alignment model will be used to generate the timestamps, otherwise you will get raw text from the MMS model\n    :param timestamps_type: Once of (`segment`, `word` or `char`) if `align` is set to True, this will be used to fragment the raw text\n    :param max_segment_len: the maximum length of the fragmented segments\n    :param cfg: configuration dict in case you want to use a custom configuration, see [CFG](#Constants.CFG)\n\n    :return: List of transcription text in the same order as input files\n    \"\"\"\n    processed_files = self._prepare_media_files(media_files)\n    cwd = os.getcwd()\n    # clone Fairseq\n    easymms_utils.clone(constants.FAIRSEQ_URL, constants.FAIRSEQ_DIR)\n    fairseq_dir = str(constants.FAIRSEQ_DIR.resolve())\n    sys.path.append(fairseq_dir)\n    os.chdir(fairseq_dir)\n    # import\n    from examples.speech_recognition.new.infer import hydra_main\n    try:\n        from fairseq.data.data_utils_fast import (\n            batch_by_size_fn,\n            batch_by_size_vec,\n            batch_fixed_shapes_fast,\n        )\n    except ImportError:\n        # we need to build the extension\n        logger.info(\"Bulding required extensions, this may take a while ...\")\n        from distutils.core import run_setup\n        run_setup(str((constants.FAIRSEQ_DIR / 'setup.py').resolve()), script_args=['build_ext', '--inplace'],\n                  stop_after='run')\n\n\n    self._setup_tmp_dir(processed_files)\n    # edit cfg\n    if cfg is None:\n        self.cfg['task']['data'] = self.cfg['decoding']['results_path'] = str(self.tmp_dir_path.resolve())\n        self.cfg['dataset']['gen_subset'] = f'{lang}:dev'\n        if device is None:\n            if torch.cuda.is_available():\n                device = 'cuda'\n            else:\n                device = 'cpu'\n        if device == 'cuda':\n            pass  # default\n        elif device == 'cpu':\n            self.cfg['common']['cpu'] = True\n        if device == 'tpu':\n            self.cfg['common']['tpu'] = True\n        cfg = OmegaConf.structured(self.cfg)\n\n    self.wer = hydra_main(cfg)\n    # get results: will just read from hypo.word as I don't want to change fairseq repo to get the hypo array\n    hypo_file = self.tmp_dir_path / constants.HYPO_WORDS_FILE\n    res = []\n    with open(hypo_file) as hw:\n        hypos = hw.readlines()\n        outputs = self._reorder_decode(hypos)\n        transcripts = [line[1].strip() for line in outputs]\n    if align:\n        align_model = AlignmentModel()\n        for i in range(len(transcripts)):\n            media_file = processed_files[i][0]\n            transcript = easymms_utils.get_transcript_segments(transcripts[i], timestamps_type, max_segment_len=max_segment_len)\n            segments = align_model.align(media_file=media_file,\n                                         transcript=transcript,\n                                         lang=lang,\n                                         device=device)\n            res.append(segments)\n    else:\n        res = transcripts\n\n    os.chdir(cwd)\n    return res\n</code></pre>"},{"location":"#easymms.models.asr.ASRModel.get_supported_langs","title":"get_supported_langs  <code>staticmethod</code>","text":"<pre><code>get_supported_langs()\n</code></pre> <p>Helper function to get supported ISO 693-3 languages by the ASR model Source https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>list of supported languages</p> Source code in <code>easymms/models/asr.py</code> <pre><code>@staticmethod\ndef get_supported_langs() -&gt; List[str]:\n\"\"\"\n    Helper function to get supported ISO 693-3 languages by the ASR model\n    Source &lt;https://dl.fbaipublicfiles.com/mms/misc/language_coverage_mms.html&gt;\n    :return: list of supported languages\n    \"\"\"\n    with open(constants.MMS_LANGS_FILE) as f:\n        data = json.load(f)\n        return [key for key in data if data[key]['ASR']]\n</code></pre>"},{"location":"#easymms.constants","title":"easymms.constants","text":"<p>Constants</p>"},{"location":"#easymms.constants.PACKAGE_NAME","title":"PACKAGE_NAME  <code>module-attribute</code>","text":"<pre><code>PACKAGE_NAME = 'easymms'\n</code></pre>"},{"location":"#easymms.constants.LOGGING_LEVEL","title":"LOGGING_LEVEL  <code>module-attribute</code>","text":"<pre><code>LOGGING_LEVEL = logging.INFO\n</code></pre>"},{"location":"#easymms.constants.PACKAGE_DATA_DIR","title":"PACKAGE_DATA_DIR  <code>module-attribute</code>","text":"<pre><code>PACKAGE_DATA_DIR = user_data_dir(PACKAGE_NAME)\n</code></pre>"},{"location":"#easymms.constants.TTS_DIR","title":"TTS_DIR  <code>module-attribute</code>","text":"<pre><code>TTS_DIR = Path(PACKAGE_DATA_DIR) / 'tts'.resolve()\n</code></pre>"},{"location":"#easymms.constants.TTS_MODELS_BASE_URL","title":"TTS_MODELS_BASE_URL  <code>module-attribute</code>","text":"<pre><code>TTS_MODELS_BASE_URL = (\n    \"https://dl.fbaipublicfiles.com/mms/tts/\"\n)\n</code></pre>"},{"location":"#easymms.constants.VITS_URL","title":"VITS_URL  <code>module-attribute</code>","text":"<pre><code>VITS_URL = 'https://github.com/jaywalnut310/vits'\n</code></pre>"},{"location":"#easymms.constants.VITS_DIR","title":"VITS_DIR  <code>module-attribute</code>","text":"<pre><code>VITS_DIR = TTS_DIR / 'vits'\n</code></pre>"},{"location":"#easymms.constants.FAIRSEQ_URL","title":"FAIRSEQ_URL  <code>module-attribute</code>","text":"<pre><code>FAIRSEQ_URL = 'https://github.com/facebookresearch/fairseq'\n</code></pre>"},{"location":"#easymms.constants.FAIRSEQ_DIR","title":"FAIRSEQ_DIR  <code>module-attribute</code>","text":"<pre><code>FAIRSEQ_DIR = Path(PACKAGE_DATA_DIR) / 'fairseq'\n</code></pre>"},{"location":"#easymms.constants.CFG","title":"CFG  <code>module-attribute</code>","text":"<pre><code>CFG = {\n    \"_name\": None,\n    \"task\": {\n        \"_name\": \"audio_finetuning\",\n        \"data\": \"\",\n        \"labels\": \"ltr\",\n    },\n    \"decoding\": {\n        \"_name\": None,\n        \"nbest\": 1,\n        \"unitlm\": False,\n        \"lmpath\": \"???\",\n        \"lexicon\": None,\n        \"beam\": 50,\n        \"beamthreshold\": 50.0,\n        \"beamsizetoken\": None,\n        \"wordscore\": -1.0,\n        \"unkweight\": -np.inf,\n        \"silweight\": 0.0,\n        \"lmweight\": 2.0,\n        \"type\": \"viterbi\",\n        \"unique_wer_file\": False,\n        \"results_path\": \"\",\n    },\n    \"common\": {\n        \"_name\": None,\n        \"no_progress_bar\": False,\n        \"log_interval\": 100,\n        \"log_format\": None,\n        \"log_file\": None,\n        \"aim_repo\": None,\n        \"aim_run_hash\": None,\n        \"tensorboard_logdir\": None,\n        \"wandb_project\": None,\n        \"azureml_logging\": False,\n        \"seed\": 1,\n        \"cpu\": False,\n        \"tpu\": False,\n        \"bf16\": False,\n        \"memory_efficient_bf16\": False,\n        \"fp16\": False,\n        \"memory_efficient_fp16\": False,\n        \"fp16_no_flatten_grads\": False,\n        \"fp16_init_scale\": 128,\n        \"fp16_scale_window\": None,\n        \"fp16_scale_tolerance\": 0.0,\n        \"on_cpu_convert_precision\": False,\n        \"min_loss_scale\": 0.0001,\n        \"threshold_loss_scale\": None,\n        \"amp\": False,\n        \"amp_batch_retries\": 2,\n        \"amp_init_scale\": 128,\n        \"amp_scale_window\": None,\n        \"user_dir\": None,\n        \"empty_cache_freq\": 0,\n        \"all_gather_list_size\": 16384,\n        \"model_parallel_size\": 1,\n        \"quantization_config_path\": None,\n        \"profile\": False,\n        \"reset_logging\": False,\n        \"suppress_crashes\": False,\n        \"use_plasma_view\": False,\n        \"plasma_path\": \"/tmp/plasma\",\n    },\n    \"common_eval\": {\n        \"_name\": None,\n        \"path\": \"\",\n        \"post_process\": \"letter\",\n        \"quiet\": False,\n        \"model_overrides\": \"{}\",\n        \"results_path\": None,\n    },\n    \"checkpoint\": {\n        \"_name\": None,\n        \"save_dir\": \"checkpoints\",\n        \"restore_file\": \"checkpoint_last.pt\",\n        \"continue_once\": None,\n        \"finetune_from_model\": None,\n        \"reset_dataloader\": False,\n        \"reset_lr_scheduler\": False,\n        \"reset_meters\": False,\n        \"reset_optimizer\": False,\n        \"optimizer_overrides\": \"{}\",\n        \"save_interval\": 1,\n        \"save_interval_updates\": 0,\n        \"keep_interval_updates\": -1,\n        \"keep_interval_updates_pattern\": -1,\n        \"keep_last_epochs\": -1,\n        \"keep_best_checkpoints\": -1,\n        \"no_save\": False,\n        \"no_epoch_checkpoints\": False,\n        \"no_last_checkpoints\": False,\n        \"no_save_optimizer_state\": False,\n        \"best_checkpoint_metric\": \"loss\",\n        \"maximize_best_checkpoint_metric\": False,\n        \"patience\": -1,\n        \"checkpoint_suffix\": \"\",\n        \"checkpoint_shard_count\": 1,\n        \"load_checkpoint_on_all_dp_ranks\": False,\n        \"write_checkpoints_asynchronously\": False,\n        \"model_parallel_size\": 1,\n    },\n    \"distributed_training\": {\n        \"_name\": None,\n        \"distributed_world_size\": 1,\n        \"distributed_num_procs\": 1,\n        \"distributed_rank\": 0,\n        \"distributed_backend\": \"nccl\",\n        \"distributed_init_method\": None,\n        \"distributed_port\": -1,\n        \"device_id\": 0,\n        \"distributed_no_spawn\": False,\n        \"ddp_backend\": \"legacy_ddp\",\n        \"ddp_comm_hook\": \"none\",\n        \"bucket_cap_mb\": 25,\n        \"fix_batches_to_gpus\": False,\n        \"find_unused_parameters\": False,\n        \"gradient_as_bucket_view\": False,\n        \"fast_stat_sync\": False,\n        \"heartbeat_timeout\": -1,\n        \"broadcast_buffers\": False,\n        \"slowmo_momentum\": None,\n        \"slowmo_base_algorithm\": \"localsgd\",\n        \"localsgd_frequency\": 3,\n        \"nprocs_per_node\": 1,\n        \"pipeline_model_parallel\": False,\n        \"pipeline_balance\": None,\n        \"pipeline_devices\": None,\n        \"pipeline_chunks\": 0,\n        \"pipeline_encoder_balance\": None,\n        \"pipeline_encoder_devices\": None,\n        \"pipeline_decoder_balance\": None,\n        \"pipeline_decoder_devices\": None,\n        \"pipeline_checkpoint\": \"never\",\n        \"zero_sharding\": \"none\",\n        \"fp16\": False,\n        \"memory_efficient_fp16\": True,\n        \"tpu\": False,\n        \"no_reshard_after_forward\": False,\n        \"fp32_reduce_scatter\": False,\n        \"cpu_offload\": False,\n        \"use_sharded_state\": False,\n        \"not_fsdp_flatten_parameters\": False,\n    },\n    \"dataset\": {\n        \"_name\": None,\n        \"num_workers\": 1,\n        \"skip_invalid_size_inputs_valid_test\": False,\n        \"max_tokens\": 4000000,\n        \"batch_size\": None,\n        \"required_batch_size_multiple\": 1,\n        \"required_seq_len_multiple\": 1,\n        \"dataset_impl\": None,\n        \"data_buffer_size\": 10,\n        \"train_subset\": \"train\",\n        \"valid_subset\": \"valid\",\n        \"combine_valid_subsets\": None,\n        \"ignore_unused_valid_subsets\": False,\n        \"validate_interval\": 1,\n        \"validate_interval_updates\": 0,\n        \"validate_after_updates\": 0,\n        \"fixed_validation_seed\": None,\n        \"disable_validation\": False,\n        \"max_tokens_valid\": 4000000,\n        \"batch_size_valid\": None,\n        \"max_valid_steps\": None,\n        \"curriculum\": 0,\n        \"gen_subset\": \"eng:dev\",\n        \"num_shards\": 1,\n        \"shard_id\": 0,\n        \"grouped_shuffling\": False,\n        \"update_epoch_batch_itr\": False,\n        \"update_ordered_indices_seed\": False,\n    },\n    \"is_ax\": False,\n}\n</code></pre>"},{"location":"#easymms.constants.HYPO_WORDS_FILE","title":"HYPO_WORDS_FILE  <code>module-attribute</code>","text":"<pre><code>HYPO_WORDS_FILE = 'hypo.word'\n</code></pre>"},{"location":"#easymms.constants.ALIGNMENT_MODEL_URL","title":"ALIGNMENT_MODEL_URL  <code>module-attribute</code>","text":"<pre><code>ALIGNMENT_MODEL_URL = \"https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/model.pt\"\n</code></pre>"},{"location":"#easymms.constants.ALIGNMENT_DICTIONARY_URL","title":"ALIGNMENT_DICTIONARY_URL  <code>module-attribute</code>","text":"<pre><code>ALIGNMENT_DICTIONARY_URL = \"https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/dictionary.txt\"\n</code></pre>"},{"location":"#easymms.constants.UROMAN_URL","title":"UROMAN_URL  <code>module-attribute</code>","text":"<pre><code>UROMAN_URL = 'https://github.com/isi-nlp/uroman'\n</code></pre>"},{"location":"#easymms.constants.UROMAN_DIR","title":"UROMAN_DIR  <code>module-attribute</code>","text":"<pre><code>UROMAN_DIR = Path(PACKAGE_DATA_DIR) / 'uroman'\n</code></pre>"},{"location":"#easymms.constants.MMS_LANGS_FILE","title":"MMS_LANGS_FILE  <code>module-attribute</code>","text":"<pre><code>MMS_LANGS_FILE = (\n    Path(__file__).parent\n    / \"data\"\n    / \"mms_langs.json\".resolve()\n)\n</code></pre>"},{"location":"#easymms.utils","title":"easymms.utils","text":"<p>Utils functions</p>"},{"location":"#easymms.utils.download_file","title":"download_file","text":"<pre><code>download_file(url, download_dir=None, chunk_size=1024)\n</code></pre> <p>Helper function to download models and other required files</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL of the file</p> required <code>download_dir</code> <p>Where to store the file</p> <code>None</code> <code>chunk_size</code> <p>size of the download chunk</p> <code>1024</code> <p>Returns:</p> Type Description <code>str</code> <p>Absolute path of the downloaded model</p> Source code in <code>easymms/utils.py</code> <pre><code>def download_file(url: str, download_dir=None, chunk_size=1024) -&gt; str:\n\"\"\"\n    Helper function to download models and other required files\n    :param url: URL of the file\n    :param download_dir: Where to store the file\n    :param chunk_size: size of the download chunk\n\n    :return: Absolute path of the downloaded model\n    \"\"\"\n\n    os.makedirs(download_dir, exist_ok=True)\n    file_name = os.path.basename(url)\n    file_path = Path(download_dir) / file_name\n    # check if the file is already there\n    if file_path.exists():\n        logging.info(f\"File '{file_name}' already exists in {download_dir}\")\n    else:\n        # download it\n        resp = requests.get(url, stream=True)\n        total = int(resp.headers.get('content-length', 0))\n\n        progress_bar = tqdm(desc=f\"Downloading File {file_name} ...\",\n                            total=total,\n                            unit='iB',\n                            unit_scale=True,\n                            unit_divisor=1024)\n\n        try:\n            with open(file_path, 'wb') as file, progress_bar:\n                for data in resp.iter_content(chunk_size=chunk_size):\n                    size = file.write(data)\n                    progress_bar.update(size)\n            logging.info(f\"Model downloaded to {file_path.absolute()}\")\n        except Exception as e:\n            # error download, just remove the file\n            os.remove(file_path)\n            raise e\n    return str(file_path.absolute())\n</code></pre>"},{"location":"#easymms.utils.download_and_extract","title":"download_and_extract","text":"<pre><code>download_and_extract(url, extract_to='.')\n</code></pre> <p>Downloads and unzips a zip folder Will be used to download uroman source code</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <p>the file URL</p> required <code>extract_to</code> <p>extract path</p> <code>'.'</code> <p>Returns:</p> Type Description <p>None</p> Source code in <code>easymms/utils.py</code> <pre><code>def download_and_extract(url, extract_to='.'):\n\"\"\"\n    Downloads and unzips a zip folder\n    Will be used to download uroman source code\n    :param url: the file URL\n    :param extract_to: extract path\n    :return: None\n    \"\"\"\n    logger.info(f\"Downloading file '{url}' and extracting to '{extract_to}' ...\")\n    http_response = urlopen(url)\n    file_name = os.path.basename(url)\n    if file_name.endswith('.zip'):\n        zipfile = ZipFile(BytesIO(http_response.read()))\n        zipfile.extractall(path=extract_to)\n    else:\n        tar = tarfile.open(BytesIO(http_response.read()), \"r:gz\")\n        tar.extractall(path=extract_to)\n        tar.close()\n</code></pre>"},{"location":"#easymms.utils.get_lang_info","title":"get_lang_info","text":"<pre><code>get_lang_info(lang)\n</code></pre> <p>Returns more info about a language,</p> <p>Parameters:</p> Name Type Description Default <code>lang</code> <code>str</code> <p>the ISO 693-3 language code</p> required <p>Returns:</p> Type Description <code>dict</code> <p>dict of info</p> Source code in <code>easymms/utils.py</code> <pre><code>def get_lang_info(lang: str) -&gt; dict:\n\"\"\"\n    Returns more info about a language,\n    :param lang: the ISO 693-3 language code\n    :return: dict of info\n    \"\"\"\n    with open(constants.MMS_LANGS_FILE) as f:\n        data = json.load(f)\n        return data[lang]\n</code></pre>"},{"location":"#easymms.utils.get_transcript_segments","title":"get_transcript_segments","text":"<pre><code>get_transcript_segments(\n    transcript, t_type=\"segment\", max_segment_len=24\n)\n</code></pre> <p>A helper function to fragment the transcript to segments  <p>Parameters:</p> Name Type Description Default <code>transcript</code> <code>str</code> <p>results of the ASR model</p> required <code>t_type</code> <code>str</code> <p>one of [<code>word</code>, <code>segment</code>, <code>char</code>]</p> <code>'segment'</code> <code>max_segment_len</code> <code>int</code> <p>the maximum length of the segment</p> <code>24</code> <p>Returns:</p> Type Description <p>list of segments</p> Source code in <code>easymms/utils.py</code> <pre><code>def get_transcript_segments(transcript: str, t_type: str = 'segment', max_segment_len: int = 24):\n\"\"\"\n    A helper function to fragment the transcript to segments\n    &lt;Quick implementation, Not perfect (barely works), needs improvements&gt;\n\n    :param transcript: results of the ASR model\n    :param t_type: one of [`word`, `segment`, `char`]\n    :param max_segment_len: the maximum length of the segment\n    :return: list of segments\n    \"\"\"\n    res = []\n    if t_type == 'word':\n        res = transcript.split()\n    elif t_type == 'char':\n        s = ''\n        for c in transcript:\n            if c == ' ':\n                continue\n            if len(s) &gt;= max_segment_len:\n                res.append(s)\n                s = c\n            else:\n                s += c\n        res.append(s)\n    else:\n        s = ''\n        words = transcript.strip().split()\n        for word in words:\n            new_word = s + ' ' + word\n            if len(new_word) &gt; max_segment_len:\n                res.append(s.strip())\n                s = word\n            else:\n                s = new_word\n        res.append(s)\n\n    return res\n</code></pre>"}]}